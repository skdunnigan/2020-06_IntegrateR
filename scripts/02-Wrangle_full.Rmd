---
title: "Session 2 - Wrangle Data"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: yeti
    highlight: tango
    df_print: paged
---

<div class="progress">
  <div class="progress-bar progress-bar-striped" role="progressbar" style="width: 45%" aria-valuenow="10" aria-valuemin="0" aria-valuemax="100"></div>
</div>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
knitr::opts_chunk$set(results = "hide")
knitr::opts_chunk$set(eval = FALSE)
```

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(tidyr)
library(lubridate)
library(here)
library(ggplot2)
```


Sometimes, the data we're working with is not quite ready for whatever it is we want to do. We may need to summarize the data based on some group, calculate additional parameters, subset the data, or even completely reshape it. And honestly, this process tends to be the largest sink in our time in many of our projects. But have no fear! The packages `dplyr` and `tidyr` can help with these tasks!  

# dplyr verbs 

There are three key functions in `dplyr` that are really the workhorses of the tidyverse:  

+  `filter()` to subset based on rows
+  `select()` to subset based on columns  
+  `mutate()` to add or modify values in columns  

We will use data downloaded from [eBird](https://ebird.org) to explore these. *eBird is a large citizen-science driven project that provides a database of bird sightings around the world. It is collaborative and the data is freely accessbile to anyone.*

First, let's read in the eBird data and explore it:  

```{r, results = 'hide'}
ebird <- read.csv(here::here("data", "eBird_workshop.csv"), stringsAsFactors = FALSE)
dplyr::glimpse(ebird)   # 183,742 rows
```

We may not talk about this in the workshop, but there are (presumably accidentally) duplicate rows in this data frame. We can see them with:  

```{r, results = "hide"}
janitor::get_dupes(ebird)   # 22 duplicates = 11 that need to go
```

They are exact copies of other rows, not any data entry problems (although `janitor::get_dupes()` is very helpful in finding errors like mis-typing a site name), so we'll just get rid of them by using `dplyr::distinct()` to keep only unique rows.  

```{r}
ebird <- dplyr::distinct(ebird)
dplyr::glimpse(ebird)   # 183,731 rows
```


## Choose _rows_: `filter()`  

This data set has a lot of rows. We might only be interested in observations about a single bird species, or from a single state, which means we want to choose *rows*. To do this, we use the verb `filter()`. The data argument comes first, then the condition.    

```{r}
filter(ebird, state == "AK")
```

Alternately, we can use the pipe, which feeds the data frame resulting from everything before it into whatever is next (read it as "and then....")  
We can also pipe to glimpse to explore it a little more.

```{r}
ebird %>% 
    filter(state == "AK") %>% 
  dplyr::glimpse()
```

Notice a couple things here:  

+  We use `==` to specify an exact condition. If we were working with numbers, we could use:  
    +  `<`  
    +  `<=`  
    +  `==`  
    +  `!=` (**not** equal to)  
    +  `>=`  
    +  `>`  
+  The condition specified inside `filter()` MUST return either true or false, for each row.  


**How about for multiple conditions?**  

We can filter based on more than one criterion, e.g. if we want all birds from Alaska, but only in the year 2008:  

```{r}
ebird %>% 
    filter(state == "AK",
           year == 2008) %>% 
head()
```


**Note**: It doesn't matter if you select `state` or `year` first - you can do either.  

**Note**: if you actually need to work with the data frame later, you may consider saving these steps as new data frames!  

**What if we want to look at birds from more than one state?**

There are a few ways to do this. 

Perhaps the easiest is to make a vector of the states you're interested in:  

```{r}
my_states <- c("AK", "AL", "MS")
```

And then use `%in%` in the filter statement, to say, e.g. "any of the states _in_ this vector": 

```{r}
ebird_mystates <- ebird %>% 
    filter(year == 2008,
           state %in% my_states) 
head(ebird_mystates)

# make sure the states we named, and only the states we named, are represented:
unique(ebird_mystates$state)
```

You could also skip the step of naming the vector separately:  

```{r}
ebird %>% 
    filter(year == 2008,
           state %in% c("AK", "AL", "MS")) %>% 
    head()
```


### Your Turn 1  

How could we pull out birds in Alaska (AK), before 2010? (Hint: you can use the same symbols on year that you would use with any other numbers)  

```{r}
ebird %>% 
  dplyr::filter(state == "AK",
                year < 2010)
```


How would you filter the data to contain only the species "American Coot" from MS and FL (your instructors' states), in all years *except* 2010? Assign this object to a data frame and verify (using `unique()`) that you did it right.    

```{r}
yt1 <- ebird %>% 
  dplyr::filter(species == "American Coot",
                state %in% c("MS", "FL"),
                year != 2010)

unique(yt1$species)
unique(yt1$state)
unique(yt1$year)
```

***

## Choose _columns_: `select()`  

When you're working with a lot of data, you may not want or need to keep all the columns you started with. In this case, `select()` is what you need.

The ebird data frame doesn't have too many columns, but that makes it easy to see exactly what we're doing! Let's pretend we only want to keep species, state, and year. We can do this two ways:  

First, by selecting the columns we **do** want to keep:  

```{r}
ebird %>% 
    select(species, state, year) %>% 
    head()
```


Or second, by using a `-` in front of the columns we **don't** want to keep:  

```{r}
ebird %>% 
    select(-samplesize, -presence) %>% 
    head()
```

Either of these can be really handy depending on what your original data frame looks like and what you're trying to do!  


Order **does** matter in `select()`; remember it didn't matter in `filter()`. In `select()`, selections are made in the order specified - so if you want to rearrange the columns of your data, you can do it with this command!  

```{r}
ebird %>% 
    select(year, state, species) %>% 
    head()
```


You can also move just one or a few columns to the beginning by specifying it/them, and then `everything()` - it keeps all other columns, in their original order:  

```{r}
ebird %>% 
    select(year, everything()) %>% 
    glimpse()
```


### Aside - helper functions  

Sometimes you have a lot of columns that start with the same prefix, or that end with the same appended matter; maybe you want to keep all of them (or get rid of all of them). In these situations, you can use the helper functions `starts_with()` and `ends_with()`. See the `dplyr` cheatsheet for other possible helper functions.  

In our little example, let's keep columns whose names start with 's':  

```{r}
ebird %>% 
    select(starts_with("s")) %>% 
    glimpse()
```

Or columns that end with 'e':  

```{r}
ebird %>% 
    select(ends_with("e")) %>% 
    glimpse()
```

*Check out the data wrangling cheat sheet for more helper functions.*

### Your Turn 2  

From the ebird data, subset to only include the species American Coot, from the states FL, AL, and MS. Keep only the state, year, and presence columns. What is the proper order of operations in this case?  

```{r}
ebird %>% 
  dplyr::filter(species == "American Coot",
                state %in% c("FL", "AL", "MS")) %>% 
  dplyr::select(state, year, presence) 

# select second because you need species and state in order to filter!
```


***  

*Setup for the next bit of the workshop*  

We're going to switch datasets here, to one that's a little more complicated: water quality data! We downloaded monitoring data from several National Estuarine Research Reserves for the year 2016, and have already winnowed it down a bit to daily averages of several parameters that we measure. 

We will use your newfound `select()` and `filter()` skills to make it an even smaller data frame, and then we'll learn how to calculate new columns and do group-wise summaries. 

```{r}
wq <- read.csv(here::here("data", "daily_wq.csv"), stringsAsFactors = FALSE)
glimpse(wq)
```

***

### Your Turn 3  

Because this is so important, I will provide working code in a chunk below. But please attempt it yourself first!!!  

We have read in the daily water quality data for a few stations. Create a new data frame called `wq_trimmed` where, from `wq`, you:    

+  **Select** the following columns:  station_code, month, day, temp, sal, do_pct, and depth.  
+  **Filter** for rows where `depth` is *not* missing. (Hint: `is.na` is the function that checks to see if a value *is* missing. How would you look for "not" `is.na`?   It's similar to "not equal to" from above.)

```{r}

```












### Your Turn 3 answer   

Last chance to try it yourself.... 
but if you didn't get it to work, run this chunk:  

```{r}
wq_trimmed <- wq %>% 
    select(station_code, month, day, temp, sal, do_pct, depth) %>% 
    filter(!is.na(depth))
```




How much has been removed from the data frame?  

```{r}
dim(wq)
dim(wq_trimmed)

# can subtract both rows and columns at once!
dim(wq) - dim(wq_trimmed)

# why did all those rows go away?
```


Some SWMP stations report `depth`, which is water depth above the data logger; and others report `level`, which is water surface relative to a standard datum (NAVD88). There is an important distinction that's irrelevant for what we want to cover today, so here we have only kept the stations that report `depth`.  



## Modifying data frames with `mutate()`  


It's easy to make a new variable out of other variables in the data frame using `mutate()`. This operates on rows. Say we want to talk about water depth (or make a graph!) to the public - we might want to change the units from meters to feet. That's simple multiplication! So here, we'll add a column to the data frame.      

```{r}
wq_trimmed %>% 
    mutate(depth_ft = depth * 3.28) %>%
  head()

```


You can also use other columns. You can use them in mathematical expressions, or just combine them:        

```{r}
wq_trimmed %>% 
    mutate(monthday = paste(month, day, sep = "-"),  # see ?paste for more info
           meaningless_thing = sal + temp) %>% 
    head()
```


You can even use a column immediately after creating it!  

```{r}
wq_trimmed <- wq_trimmed %>% 
    mutate(monthday = paste(month, day, sep = "-"),
           meaningless_thing = sal + temp,
           even_more_meaningless_thing = meaningless_thing + 5) 

```




### Your Turn 4  

There are two parts to this. You can approach them separately or within the same series of pipes. Remember to save the result as the new, better, `wq_trimmed` data frame!   

1.  Now that we've started creating more columns, it might make sense to get rid of some old ones. Remove `monthday` and `meaningless_thing` from the `wq_trimmed` data frame.  
2.  The same person that wants to see `depth` in feet rather than meters *also* wants you to turn `temp` into Fahrenheit, from Celsius. You've looked up the conversion. Now create a new column, `temp_f`, with the new variable.   **F = (9/5)(temp in C) + 32**


As with Your Turn 4, the answer is below. No peeking!  


```{r}

```










### Your Turn 4 answer  

```{r}
wq_trimmed <- wq_trimmed %>% 
    select(-monthday, -meaningless_thing) %>% 
    mutate(temp_f = (9/5) * temp + 32)
View(wq_trimmed)
```





# Group-wise operations with `group_by()` and `summarize()`  

Say we want to find out how many birds were seen by state in the ebird dataset. We can do that. These operations are also great for lumping data into daily, monthly, or yearly averages, which we'll do on the SWMP dataset.    

```{r}
ebird %>% 
    group_by(state) %>% 
    summarize(mean_presence = mean(presence, na.rm = TRUE),
              max_presence = max(presence, na.rm = TRUE)) %>% 
    View()
```



We can also group by combinations of multiple variables:  

```{r}
ebird %>% 
    group_by(state, species) %>% 
    summarize(mean_presence = mean(presence, na.rm = TRUE),
              max_presence = max(presence, na.rm = TRUE)) %>% 
    View()
```



### Your Turn 5  

How would you group the `wq_trimmed` dataset to calculate monthly average temperature and salinity, and their standard deviations, at each station?  

```{r}

```











### Your Turn 5 answer  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize(mean_temp = mean(temp, na.rm = TRUE),
              sd_temp = sd(temp, na.rm = TRUE),
              mean_sal = mean(sal, na.rm = TRUE),
              sd_sal = sd(sal, na.rm = TRUE)) %>% 
    View()
```


### Aside - more summarize options  

There are a couple of useful variants of `summarize` for special cases. Note that you can only perform one summary function with these (so you couldn't calculate mean *and* standard deviation in this way).    

#### `summarize_all()`  

Applies a function to all of the non-grouping variables! (Even "day", which isn't meaningful, but that's okay)  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_all(mean, na.rm = TRUE) %>% 
    View()
```


#### `summarize_at()`  

To choose which variables to summarize  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_at(c("temp", "sal"), mean, na.rm = TRUE) %>% 
    View()
```


# Sort  


You can also sort your data frame (or its summary) using `arrange()`. Let's put our ebird summary in order by species, then state:  

```{r}
ebird %>% 
    group_by(state, species) %>% 
    summarize(mean_presence = mean(presence, na.rm = TRUE),
              max_presence = max(presence, na.rm = TRUE)) %>% 
    arrange(species, state) %>% 
    View()
```


Or put it in order by `max_presence`:  

```{r}
ebird %>% 
    group_by(state, species) %>% 
    summarize(mean_presence = mean(presence, na.rm = TRUE),
              max_presence = max(presence, na.rm = TRUE)) %>% 
    arrange(max_presence) %>% 
    View()
```


If you want the highest number at the top instead, use `desc()`:  

```{r}
ebird %>% 
    group_by(state, species) %>% 
    summarize(mean_presence = mean(presence, na.rm = TRUE),
              max_presence = max(presence, na.rm = TRUE)) %>% 
    arrange(desc(max_presence)) %>% 
    View()
```




### Your Turn 6  

Use the `wq_trimmed` data frame. Calculate monthly average temp, sal, and do_pct (at least - more variables if you like) *for each station*. Make a scatterplot using any two of these variables. Use what you've learned about `ggplot2`'s options to adjust the look and feel of the graph. Is the relationship what you expected it to be? Does it vary by site?     

```{r}

```













Here's one possible way to go about it. You could also make a new data frame of the summary variables, and use that in a separate call to `ggplot`.  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_all(mean, na.rm = TRUE) %>% 
    ggplot() +
    geom_point(aes(x = sal, y = do_pct, col = station_code), 
               size = 3, alpha = 0.3) +
    theme_minimal() +
    facet_wrap(~station_code)
```

# Reshaping data with `tidyr`

Run the chunk below to "reset" each of the tester data frames

```{r}
# load and clean up some data files we've already been working with
ebird <- read.csv(here::here("data", "eBird_workshop.csv"), stringsAsFactors = FALSE)
ebird <- dplyr::distinct(ebird)

wq <- read.csv(here::here("data", "daily_wq.csv"), stringsAsFactors = FALSE)
wq_trimmed <- wq %>%
    select(station_code, month, day, temp, sal, do_pct, depth) %>%
    filter(!is.na(depth)) %>%
    mutate(depth_ft = depth * 3.28,
           temp_f = (9/5) * temp + 32)
```

## Example data

tables 1, 2, 3, 4a, 4b, and 5 are all built-in datasets in R. They are different ways of representing the same data and are subsets from World Health Organization Tuberculosis data between 1999-2000.

```{r}
table1

table2

```

Generate "toy data" to follow along with the slides

```{r}
cases <- tribble(
    ~Country, ~"2011", ~"2012", ~"2013",
    "FR",    7000,    6900,    7000,
    "DE",    5800,    6000,    6200,
    "US",   15000,   14000,   13000
)

pollution <- tribble(
    ~city, ~size, ~amount,
    "New York", "large",      23,
    "New York", "small",      14,
    "London", "large",      22,
    "London", "small",      16,
    "Beijing", "large",     121,
    "Beijing", "small",     121
)
```

### Your Turn 7

*not code*
On a sheet of paper, draw how the cases data set would look if it had the same values grouped into three columns: country, year, n


## Pivoting data from wide to long using `pivot_longer()`

```{r}



```

### Your Turn 8

Use `pivot_longer()` to reorganize `table4a` into three columns: country, year, and cases.

```{r}
table4a

```


## Pivoting data from long to wide using `pivot_wider()`


```{r}


```


### Your Turn 9

Use pivot_wider() to reorganize table2 into four columns: country, year, cases, and population.

```{r}
table2

```



***
***
***

# Extra OYO


## Dates and Times  

Because we've just worked a little bit with mutate, and one of the things you may find yourself doing is trying to get dates and times in the right format, this seems like a good time to tackle it. We will make up a vector of date-like things and learn how to use the `lubridate` package to deal with them.  

```{r}
dates_df <- data.frame(dates01 = c("1-1-2019", "1-2-2019", "1-3-2019"),
                       dates02 = c("1/1/19", "1/2/19", "1/3/19"),
                       dates03 = c("2019-01-01", "2019-01-02", "2019-01-03"),
                       datetime = c("2019-01-01 12:00", "2019-01-01 13:00", 
                                    "2019-01-01 14:00"))

```

How are these showing up in the data frame?  

```{r}
glimpse(dates_df)
```

If you try to make a time-series graph, it might work; but it will be very, very slow when you have more than 3 values. You also run the risk of dates being shuffled so they're out of order. So we really want those dates to be *dates*.  

This seems like it should be easy in base R, using the `as.Date()` function, but that can give you the wrong output (without so much as a warning!) if your date isn't formatted the way the function thinks it should be:  

```{r}
dates_df %>% 
    mutate(dates01_changed = as.Date(dates01),
           dates02_changed = as.Date(dates02),
           dates03_changed = as.Date(dates03))
```

Enter the lubridate package. It lets you turn character strings and even numbers into dates - or date/times! - by specifying the order of what's in your data as the function name.  

That's hard to explain, so let's just do it. The first column is currently in "m-d-y" (with 4 digits in the year) format. The second is "m/d/y" - similar format, but only two digits in the year. The same lubridate function will work for both of these: `mdy()`. 

```{r}
dates_df %>% 
    mutate(dates01_changed = mdy(dates01),
           dates02_changed = mdy(dates02)) %>% 
    glimpse()  # to see the structure
```

Notice the returned date is "yyyy-mm-dd", which is actually a standard date format known as ISO 8601.  

For our 3rd column, it's already in that order we need, but we need it to be a date. Any guesses as to what that function is?  

```{r, eval = FALSE}
dates_df %>% 
    mutate(dates01_changed = mdy(dates01),
           dates02_changed = mdy(dates02),
           dates03_changed = ---(dates03)) %>% 
    glimpse()
```


Times make things trickier. R wants date-time columns to be in a format called `POSIX`. There is a function in base R called `as.POSIXct()` that lets you transform different values into that format. It is a burden though to specify the format you're feeding into the function. (If you don't want to take my word for it, look up the help file:  `?as.POSIXct`)

The lubridate package makes it just as simple as the dates we did above. Again, the exact function you want to use depends on the order your date appears, but all you have to do is add `_hm` or `_hms` to the end of your `mdy`/`ymd` function, depending on whether your time has seconds or not.  

```{r}
dates_df %>% 
    mutate(dates01_changed = mdy(dates01),
           dates02_changed = mdy(dates02),
           dates03_changed = ymd(dates03),
           datetime_changed = ymd_hm(datetime)) %>% 
    glimpse()
```


If you only work with times in their own column, the `hms` library may help you out. There are differences between operating systems and software versions in how Excel stores dates and times in its brain, so **be VERY CAREFUL** and check your transformed data to make sure you have the output you think you have!  


### Your Turn - Dates and Times

`mutate` can be used to add a complete vector of the same value. That is what we'll do below. Fill in the skeleton code to create a column for a full yyyy-mm-dd style date, then make a *line* graph of temperature throughout the year at the water quality stations, and color them by station code.  

Which of these stations do you think is at the NERR in Alaska?  

```{r, eval = FALSE}
wq_trimmed %>% 
    mutate(year = 2016,
           date_pasted = paste(year, month, day, sep = "-"),
           date = ----(date_pasted)) %>%   # hint: use a lubridate function!
    ggplot() +
    geom_----(mapping = ----(x = date, y = temp_f, col = ----))
```



***  
***  
***

